{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (5,5)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(1,1),(2,1),(2,3)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [-10, -10, -10, -10, 10] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = 0\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (3,0)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [0.8, 0.1, 0.0 , 0.1]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 22 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "###########################################         \n",
    "    \n",
    "                \n",
    "                        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAACBCAYAAADpLPAWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAHa0lEQVR4nO3dzWtc5x3F8XM6VhRqdaPEm9iiKrSLetWAcAvZlGSRl4Zmm5Rm600DdnEp6R/Rkk02pg0UGgiFZFFCQIS8LLqoG8U1BVfEmJBiV4E69iKxoLal/rqQAko98tw7us+9Pz36fkCgNx4ddODoMszoOiIEAMjra0MHAADcG0MNAMkx1ACQHEMNAMkx1ACQ3KESh97n2bhfh0scjRb+o3Xdjlvu6rwH50exuDDT1XGY0idX7uizG5ud9TqaOxyH5ue7Og5T2rhxQ5s318f2WmSo79dhfd+PlTgaLZyLdzo9b3FhRn9dXuj0TLR34vErnZ53aH5eD5053emZaG/t1y/t+jUe+gCA5BhqAEiOoQaA5BhqAEiOoQaA5BoNte0nbH9k+7LtF0uHQj/otU70Wp+JQ217JOllSU9KOi7pOdvHSwdDWfRaJ3qtU5Mr6hOSLkfExxFxW9Jrkp4pGws9oNc60WuFmgz1UUk7n2F/dftzX2H7pO0V2yt3dKurfCinda/Xrm/2Fg5Ta93r5s313sJhOk2GetxLGu+620BEnI2IpYhYmtHs3pOhtNa9Hnlg1EMs7FHrXkdz/LuH7JoM9VVJO183fEzSWpk46BG91oleK9RkqD+Q9B3b37J9n6RnJf2pbCz0gF7rRK8VmvhPmSJiw/YLkpYljSS9EhEXiydDUfRaJ3qtU6P/nhcRb0l6q3AW9Ixe60Sv9eGViQCQHEMNAMkx1ACQHEMNAMkVuRVX15bXLnR63uMPfa/T8wCgJK6oASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASC5fXHPxMy4nyOA0riiBoDkGGoASI6hBoDkGGoASI6hBoDkJg617QXb79letX3R9qk+gqEseq0TvdapydPzNiSdiYjztr8h6UPbb0fEPwpnQ1n0Wid6rdDEK+qI+DQizm+//4WkVUlHSwdDWfRaJ3qtU6vHqG0vSnpY0rkSYTAMeq0Tvdaj8VDbnpP0uqTTEfH5mK+ftL1ie+WObnWZEQW16fXa9c3+A2IqbXrdvLnef0C00miobc9oq/RXI+KNcd8TEWcjYikilmY022VGFNK21yMPjPoNiKm07XU0d7jfgGitybM+LOl3klYj4jflI6EP9Foneq1TkyvqRyQ9L+lR2xe2354qnAvl0Wud6LVCE5+eFxF/luQesqBH9Foneq0Tr0wEgOQYagBIjqEGgOQYagBIbl/ciivz7akyZwNQB66oASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASA5hhoAkmOoASC5fXHPRORw6e9f5x6RCVyK652eN3tlXd/++V86PfOgWF670NlZJ165tuvXuKIGgOQYagBIjqEGgOQYagBIjqEGgOQYagBIrvFQ2x7Z/pvtN0sGQr/otU70Wpc2V9SnJK2WCoLB0Gud6LUijYba9jFJP5L027Jx0Cd6rRO91qfpFfVLkn4p6b+7fYPtk7ZXbK/c0a1OwqE4eq0TvVZm4lDbflrSvyPiw3t9X0ScjYiliFia0WxnAVEGvdaJXuvU5Ir6EUk/tv2JpNckPWr7D0VToQ/0Wid6rdDEoY6IX0XEsYhYlPSspHcj4qfFk6Eoeq0TvdaJ51EDQHKt/s1pRLwv6f0iSTAYeq0TvdaDK2oASI6hBoDkGGoASI6hBoDkHBHdH2pfk/TPCd/2oKTPOv/h3cmcr2m2b0bEka5+aMNepTp+d0Og1+llziY1y7drr0WGugnbKxGxNMgPbyBzvszZpNz5yDa9zPkyZ5P2no+HPgAgOYYaAJIbcqjPDvizm8icL3M2KXc+sk0vc77M2aQ95hvsMWoAQDM89AEAyTHUAJDcIENt+wnbH9m+bPvFITKMY3vB9nu2V21ftH1q6EzjZL1xadZepf3RLb22d1B67X2obY8kvSzpSUnHJT1n+3jfOXaxIelMRHxX0g8k/SxRtp3S3bg0ea/S/uiWXts7EL0OcUV9QtLliPg4Im5r6y4UzwyQ4y4R8WlEnN9+/wtt/XKPDpvqqxLfuDRtr1L+bul1Ogel1yGG+qikKzs+vqpEv9gv2V6U9LCkc8MmucvEG5cOZF/0KqXtll73qOZehxhqj/lcqucI2p6T9Lqk0xHx+dB5vtT0xqUDSd+rlLNbet272nsdYqivSlrY8fExSWsD5BjL9oy2Cn81It4YOs//yXzj0tS9Sqm7pdc9OAi99v6CF9uHJF2S9Jikf0n6QNJPIuJir0HGsG1Jv5d0IyJOD53nXmz/UNIvIuLpobNIuXuV9k+39NrOQem19yvqiNiQ9IKkZW098P/HLKVr6y/g89r6y3dh++2poUPtB8l7leh2KvSaAy8hB4DkeGUiACTHUANAcgw1ACTHUANAcgw1ACTHUANAcgw1ACT3P9QVgEviV6etAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Policy is : [[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n",
      "Gamma =  0.0\n",
      "The value of that policy is : \n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -2.5 -2.5\n",
      " -2.5 -2.5  2.5  0.   0.   0.   0.   0. ]\n",
      "Gamma =  0.1\n",
      "The value of that policy is : \n",
      " [-4.68750000e-05 -1.95312500e-06 -4.49218750e-05  0.00000000e+00\n",
      "  4.39453125e-05 -1.82421875e-03 -1.77734375e-03 -1.95312500e-06\n",
      "  1.68847656e-03 -6.93876953e-02 -6.93427734e-02  6.58320313e-02\n",
      " -2.63508789e+00 -2.69918848e+00 -2.63339844e+00 -2.56752246e+00\n",
      "  2.49995703e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.2\n",
      "The value of that policy is : \n",
      " [-4.82304688e-04 -4.96093750e-05 -4.32968750e-04 -3.32031250e-06\n",
      "  4.07773438e-04 -8.67203125e-03 -8.19414063e-03 -4.40234375e-05\n",
      "  7.34917969e-03 -1.55684258e-01 -1.55249297e-01  1.39275273e-01\n",
      " -2.79373105e+00 -2.92526523e+00 -2.78635703e+00 -2.64666957e+00\n",
      "  2.49961121e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.3\n",
      "The value of that policy is : \n",
      " [-2.09570856e-03 -3.32989151e-04 -1.76823674e-03 -3.41607513e-05\n",
      "  1.59423926e-03 -2.35658408e-02 -2.15151868e-02 -2.75249954e-04\n",
      "  1.81110490e-02 -2.65211578e-01 -2.63424913e-01  2.22054185e-01\n",
      " -2.98242366e+00 -3.18484646e+00 -2.96414549e+00 -2.74045175e+00\n",
      "  2.49851009e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.4\n",
      "The value of that policy is : \n",
      " [-6.63088000e-03 -1.46531750e-03 -5.22071250e-03 -2.07917500e-04\n",
      "  4.41662500e-03 -5.17681475e-02 -4.54072200e-02 -1.11453500e-03\n",
      "  3.55278700e-02 -4.07757463e-01 -4.02441125e-01  3.16432445e-01\n",
      " -3.21053400e+00 -3.48718913e+00 -3.17426842e+00 -2.85314402e+00\n",
      "  2.49592332e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.5\n",
      "The value of that policy is : \n",
      " [-1.79691581e-02 -5.16507080e-03 -1.31506607e-02 -9.37459990e-04\n",
      "  1.01341321e-02 -1.02817748e-01 -8.60419458e-02 -3.60220482e-03\n",
      "  6.17080105e-02 -5.99126039e-01 -5.85623635e-01  4.25392403e-01\n",
      " -3.49210435e+00 -3.84569071e+00 -3.42778558e+00 -2.99101887e+00\n",
      "  2.49062710e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.6\n",
      "The value of that policy is : \n",
      " [-4.52800926e-02 -1.62402717e-02 -3.07501561e-02 -3.60876225e-03\n",
      "  2.05388188e-02 -1.95361699e-01 -1.54571981e-01 -1.03591587e-02\n",
      "  9.93671122e-02 -8.66712753e-01 -8.34938078e-01  5.52822451e-01\n",
      " -3.84954607e+00 -4.28083533e+00 -3.74192493e+00 -3.16378843e+00\n",
      "  2.48042274e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n",
      "Gamma =  0.7\n",
      "The value of that policy is : \n",
      " [-0.11301629 -0.04956894 -0.07134237 -0.01335025  0.03707705 -0.37048225\n",
      " -0.27359335 -0.02882115  0.15093775 -1.2633235  -1.18977728  0.70319872\n",
      " -4.32206048 -4.82648008 -4.14568092 -3.3876804   2.46087434  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Gamma =  0.8\n",
      "The value of that policy is : \n",
      " [-0.29757302 -0.15806191 -0.17692912 -0.05197577  0.05329898 -0.73498697\n",
      " -0.49794293 -0.08451179  0.21165338 -1.90768314 -1.73054432  0.87763645\n",
      " -4.98827552 -5.54551344 -4.69383254 -3.6931654   2.42113156  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Gamma =  0.9\n",
      "The value of that policy is : \n",
      " [-0.93341213 -0.60635888 -0.54917593 -0.25725748 -0.02265368 -1.6756746\n",
      " -1.02834632 -0.31463245  0.2015405  -3.16298344 -2.67857924  1.03118118\n",
      " -6.05627306 -6.58649569 -5.51949554 -4.15498826  2.31892092  0.\n",
      "  0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "grid = GridWorld()\n",
    "\n",
    "# first plot is walls/ squares that can be walked in\n",
    "# second plot is absorbing states (all bottom row are absorbing states)\n",
    "# third plot is rewardsn(bottom right is positive reward, rest of bottom row are negative)\n",
    "\n",
    "### Question 1 : Change the policy here:\n",
    "uniform_policy = np.ones((grid.state_size, grid.action_size)) * 1/4\n",
    "print(\"The Policy is : {}\".format(uniform_policy))\n",
    "\n",
    "for i in range(0,10):\n",
    "    val = grid.policy_evaluation(uniform_policy,0.0001,i/10) # will return vector of value of each state\n",
    "    print(\"Gamma = \",i/10)\n",
    "    print(\"The value of that policy is : \\n {}\".format(val))\n",
    "\n",
    "# as gamma is increased, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAMD0lEQVR4nO3dX4idd53H8c9n/mRqJylNY2zaTslItrgGwRaHWM2FkBWMVRRhoQpmwQoBraSCoBV6I+SiVxIC7UUwpRal4m6kK71pC2lXllp1um03xkS2K4mdpNPZTRtMMuPMJOfrxYzmjzOZZzLnOb/zfPt+QWAmZzjnw5Pz7jM5kz7HESEAefSUHgCgvYgaSIaogWSIGkiGqIFk+uq40941g9G3bm0ddw1A0vlT7+jCmXNe6LZaou5bt1YbHtpVx10DkDS+e++it/HtN5AMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEylqG1vt/1726/bfrDuUQtpTU5p5viJEg99TZq2t0madGxLbF0yatu9kh6R9GlJmyV9yfbmuoddqjU5pYk9+zX+8KOaOnS0kw99TZq2t0madGxLba1ypt4i6fWI+ENEzEj6iaTP1zvrcqeeOKCBTRt13T9u0umnntX5U+908uGXrQl7o9XS5Gu/Kz1j2ZpwbP+q1NYqUd8m6Y1LPh+b/73L2N5pe9T26IUz59q1T5K07r57df1H71LvmtXa8ODX1O1XKu32vdFq6e3H/1XT/3Os9JRl6/Zje6lSW6tcTXShy5D+3bvqRcQ+SfskaWB4qK3vutezqv/imP7+q3xld+j2vWf/4yWde+kV9d/yPr15xbeFfe97r9bf/y+Fli2t24/tpUptrRL1mKTbL/l8SNLJeuagEwY/9hFNjh7S4NYRrf74R0rPQZtV+fb7N5LusP1+26skfVHSz+udhTr1XDeg9bu+otaZs6WnoAZLnqkj4rztb0h6RlKvpMci4nDty1CrnoFVuuFTnyg9AzVwHW86PzA8FLxDB1Cf8d17NX1sbMG33eFflAHJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEyVa5Sl1rd6tvSEZTl/trsvtofyOFMDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJLBm17cdsT9j+bScGLaY1OaWZ4ydKTkirNTmlmTdOlp5RWZOeCyW2VjlTPy5pe807rqo1OaWJPfs1/vCjmjp0tOSUlGYnTunMc/9ZekYlTXoulNq65DXKIuIXtofrn7K4U08c0MCmjeoZvF6nn3pW/bferL51a0tOQiFNei6U2tqICw+uu+9ezZ58S2cPvqj1X98h93PxvXerJj0XSm1t2wtltnfaHrU9euHMuXbdrSSpZ9XFg9HNf4ioX5OeC6W2ti3qiNgXESMRMdK7ZrBdd4uaTf/vcSli7uNjY4rZ84UXYaX4kda73OQrh/XOk/+umT+e0Ns//De1pqZKT8IKVfmR1pOSfinpA7bHbH+1/lnolLX/fI8G7hhWzM5q/a6vqPeGNaUnYYUc8996tdPA8FBseGhX2++3DrxDx5xoteQevnFrivHdezV9bMwL3cafIiSJoBPhTxJIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSKYRVxNFczXpIhR1XYCi0zhTA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kMySUdu+3fbzto/YPmz7gU4Mu1Jrckozx0+UeOj0OLb1KXFsq5ypz0v6VkR8UNLdku63vbneWZdrTU5pYs9+jT/8qKYOHe3kQ6fHsa1PqWO75IUHI+JNSW/Of3zG9hFJt0n6Xc3b/ubUEwc0sGmjegav1+mnnlX/rTerb93aTj18ahzb+pQ6tsu6mqjtYUl3SfrVArftlLRTknpvurEN0y5ad9+9mj35ls4efFHrv75D7s9x1cduwLGtT6ljW/mFMturJR2Q9M2I+NOVt0fEvogYiYiR3jWD7dyonlUXDwZPuvbi2Nan1LGtFLXtfs0F/eOI+Fm9kwCsRJVXvy1pv6QjEfH9+icBWIkqZ+qtknZI2mb71flf99S8C8A1ckS0/U4Hhodiw0O72n6/dWjS28JIzXtrmCYd3yYd2/HdezV9bMwL3ca/KAOSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIBmiBpIhaiAZogaSIWogGaIGkiFqIJllXU00oyb9j/FNxPHtPM7UQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMkQNJEPUQDJEDSRD1EAyRA0kQ9RAMktGbfs627+2/Zrtw7a/14lhV2pNTmnm+IkSD31NmrS3SVulZu0tsbXKmXpa0raI+LCkOyVtt313vbMu15qc0sSe/Rp/+FFNHTrayYe+Jk3a26StUrP2ltq65DXKIiIknZ3/tH/+V9Q56kqnnjiggU0b1TN4vU4/9az6b71ZfevWdnLCsjRpb5O2Ss3aW2qr55pd4ovsXkkvS/oHSY9ExHeu9vUDw0Ox4aFd7VkoqTUzq9mTb+nswRd1044vyP3dfTG7Ju1t0lapWXvr3Dq+e6+mj415odsqvVAWERci4k5JQ5K22P7QlV9je6ftUdujF86cW9niK0euungwuvkP8a+atLdJW6Vm7S21dVmvfkfEaUkvSNq+wG37ImIkIkZ61wy2aR6A5ary6vd62zfOf/weSZ+U1N2vUADvYlUu5n+LpB/O/726R9JPI+LpemcBuFaVXihbrna/UAbgcit+oQxAcxA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZBMLRdJuME3xUf9T22/X6BOz5x8tfSEyrZ86g2NvvZnLpIAvBsQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kAxRA8kQNZAMUQPJEDWQDFEDyRA1kEzlqG332n7F9tN1DgKwMss5Uz8g6UhdQwC0R6WobQ9J+oykH9Q7B8BKVT1T75H0bUmtxb7A9k7bo7ZHZzXdlnEAlm/JqG1/VtJERLx8ta+LiH0RMRIRI/0aaNtAAMtT5Uy9VdLnbB+T9BNJ22z/qNZVAK7ZklFHxHcjYigihiV9UdLBiPhy7csAXBN+Tg0k07ecL46IFyS9UMsSAG3BmRpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWSIGkiGqIFkiBpIhqiBZIgaSIaogWQcEe2/U/v/JB1v892+V9L/t/k+69SkvU3aKjVrb11bN0bE+oVuqCXqOtgejYiR0juqatLeJm2VmrW3xFa+/QaSIWogmSZFva/0gGVq0t4mbZWatbfjWxvzd2oA1TTpTA2gAqIGkmlE1La32/697ddtP1h6z9XYfsz2hO3flt6yFNu3237e9hHbh20/UHrTYmxfZ/vXtl+b3/q90puqsN1r+xXbT3fqMbs+atu9kh6R9GlJmyV9yfbmsquu6nFJ20uPqOi8pG9FxAcl3S3p/i4+ttOStkXEhyXdKWm77bsLb6riAUlHOvmAXR+1pC2SXo+IP0TEjObeefPzhTctKiJ+Ient0juqiIg3I+K/5j8+o7kn321lVy0s5pyd/7R//ldXv8pre0jSZyT9oJOP24Sob5P0xiWfj6lLn3hNZntY0l2SflV2yeLmv5V9VdKEpOciomu3ztsj6duSWp180CZE7QV+r6v/C900tldLOiDpmxHxp9J7FhMRFyLiTklDkrbY/lDpTYux/VlJExHxcqcfuwlRj0m6/ZLPhySdLLQlHdv9mgv6xxHxs9J7qoiI05p799Vufu1iq6TP2T6mub8ybrP9o048cBOi/o2kO2y/3/Yqzb3x/c8Lb0rBtiXtl3QkIr5fes/V2F5v+8b5j98j6ZOSjpZdtbiI+G5EDEXEsOaeswcj4sudeOyujzoizkv6hqRnNPdCzk8j4nDZVYuz/aSkX0r6gO0x218tvekqtkraobmzyKvzv+4pPWoRt0h63vZ/a+4/9M9FRMd+TNQk/DNRIJmuP1MDWB6iBpIhaiAZogaSIWogGaIGkiFqIJm/AIpJ6UN8JqXhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using draw_deterministic_policy to illustrate some arbitracy policy.\n",
    "Policy2 = np.zeros(22).astype(int)\n",
    "Policy2[2] = 3 # array 'Policy2' consists of action to be taken at each state, 0 is North, 1 is East, 2 is South and 3 is West\n",
    "Policy2[6] = 2\n",
    "Policy2[18] = 1\n",
    "grid.draw_deterministic_policy(Policy2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Question 2 - greedy actions through Policy Improvement Theorem - find optimal policy\n",
    "\n",
    "optimal_policy = \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
